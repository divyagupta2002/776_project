{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from tensorflow.python import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "#from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.optimizer_v2 import adam\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout, Bidirectional\n",
    "#from tensorflow.keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merging import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.efficientnet_v2 import EfficientNetV2L\n",
    "from keras.applications.efficientnet_v2 import preprocess_input\n",
    "from keras.models import Model\n",
    "#from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model for feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr\n",
    "model_EfficientNetV2L = EfficientNetV2L(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg',\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_efficientnet_v2 = Model(model_EfficientNetV2L.input, model_EfficientNetV2L.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "caption_model_v1 = load_model('EfficientNetV2L_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 74\n",
    "vocab = open('./vocab.txt', 'r').read().strip().split('\\n')\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "vocab_size = len(ixtoword) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(480, 480))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image) \n",
    "    fea_vec = model_efficientnet_v2.predict(image) \n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
    "    return fea_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySearch_v1(photo, model):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        # print(sequence)\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        sequence = np.array(sequence)\n",
    "        # print(sequence)\n",
    "        yhat = model.predict([photo,sequence], verbose=0) #here we must pass the encoded image otw error show failed to find data adapter\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_predictions_v1(image, model, beam_index = 3):\n",
    "    start = [wordtoix[\"startseq\"]]\n",
    "    start_word = [[start, 0.0]]\n",
    "    while len(start_word[0][0]) < max_length:\n",
    "        temp = []\n",
    "        for s in start_word:\n",
    "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
    "            preds = model.predict([image, par_caps], verbose=0)\n",
    "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
    "            # Getting the top <beam_index>(n) predictions and creating a \n",
    "            # new list so as to put them via the model again\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "                prob += preds[0][w]\n",
    "                temp.append([next_cap, prob])\n",
    "                    \n",
    "        start_word = temp\n",
    "        # Sorting according to the probabilities\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        # Getting the top words\n",
    "        start_word = start_word[-beam_index:]\n",
    "    \n",
    "    start_word = start_word[-1][0]\n",
    "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
    "    final_caption = []\n",
    "    \n",
    "    for i in intermediate_caption:\n",
    "        if i != 'endseq':\n",
    "            final_caption.append(i)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = os.listdir(r\"../testingImage/\")\n",
    "image_path = '../testingImage/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(image_path):\n",
    "    img = preprocess(image_path)\n",
    "    pre_img = plt.imshow(img[0])\n",
    "    plt.savefig('image4.jpg')\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(img)\n",
    "      feature_vector = model_efficientnet_v2(img)\n",
    "      # get loss as dot product of feature vector\n",
    "      loss = tf.math.reduce_sum(feature_vector*feature_vector)\n",
    "      # get gradient\n",
    "\n",
    "    gradient = tape.gradient(loss, img)[0]\n",
    "    alpha = gradient\n",
    "    gradient = tf.math.reduce_mean(tf.math.abs(gradient), axis=-1)\n",
    "\n",
    "    # plot\n",
    "    saliency_img = plt.imshow(gradient, cmap='jet')\n",
    "\n",
    "    # save the image to disk\n",
    "    plt.savefig('image2.jpg')\n",
    "\n",
    "\n",
    "    # get absolute values of alpha\n",
    "    alpha = tf.math.abs(alpha) \n",
    "    # scale alpha to [0, 1]\n",
    "    alpha = alpha / tf.math.reduce_max(alpha)\n",
    "\n",
    "    # all values below threshold are set to 0 and all values above are set to 1\n",
    "\n",
    "    alpha = tf.where(alpha >= 0.05, 1.0, 0.0)\n",
    "    img = tf.reshape(img, (480, 480, 3))\n",
    "\n",
    "    new_img2 = img + tf.random.uniform(img.shape, minval = 50, maxval = 70)*alpha\n",
    "\n",
    "    # conevrt this to ndarray of shape (1, 480, 480, 3) and float32\n",
    "    new_img2 = tf.reshape(new_img2, (1, 480, 480, 3))\n",
    "    new_img2 = np.array(new_img2)\n",
    "    new_img2 = new_img2.astype('float32')\n",
    "    # plot\n",
    "    ace_img = plt.imshow(new_img2[0])\n",
    "\n",
    "    # save the image to disk\n",
    "    plt.savefig('image3.jpg')\n",
    "\n",
    "    fea_vec = model_efficientnet_v2.predict(new_img2) \n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
    "    fea_vec = fea_vec.reshape(1, 1280)\n",
    "    acap2 = beam_search_predictions_v1(fea_vec, caption_model_v1)\n",
    "    acap1 = greedySearch_v1(fea_vec, caption_model_v1)\n",
    "    img = preprocess(image_path)\n",
    "    plt.imshow(img[0])\n",
    "\n",
    "    model = model_efficientnet_v2\n",
    "\n",
    "    last_conv_layer_name = \"block7g_add\"\n",
    "\n",
    "    grad_model = tf.keras.models.Model(\n",
    "      [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img)\n",
    "        pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    jet_heatmap = image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = image.img_to_array(jet_heatmap)\n",
    "\n",
    "    superimposed_img_1 = (-1)*jet_heatmap * 0.4 + img\n",
    "    superimposed_img_1=plt.imshow(superimposed_img_1[0])\n",
    "    # save the image to disk\n",
    "    plt.savefig('image1.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    superimposed_img_2 = jet_heatmap * 0.4 + img\n",
    "    superimposed_img_2 = image.array_to_img(superimposed_img_2[0])\n",
    "    superimposed_img_2= plt.imshow(superimposed_img_2)\n",
    "    return superimposed_img_1, acap1, acap2, saliency_img, ace_img, pre_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ghost.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m full_img_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mghost.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m encoded_img1 \u001b[39m=\u001b[39m encode( full_img_path )\n\u001b[0;32m      3\u001b[0m grad_cam, a_cap1, a_cap2, saliency_img, ace_img, pre_img \u001b[39m=\u001b[39m explain(full_img_path)\n\u001b[0;32m      4\u001b[0m encoded_img1 \u001b[39m=\u001b[39m encoded_img1\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m1280\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m     image \u001b[39m=\u001b[39m preprocess(image) \n\u001b[0;32m      3\u001b[0m     fea_vec \u001b[39m=\u001b[39m model_efficientnet_v2\u001b[39m.\u001b[39mpredict(image) \n\u001b[0;32m      4\u001b[0m     fea_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(fea_vec, fea_vec\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(image_path):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mload_img(image_path, target_size\u001b[39m=\u001b[39;49m(\u001b[39m480\u001b[39;49m, \u001b[39m480\u001b[39;49m))\n\u001b[0;32m      3\u001b[0m     x \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mimg_to_array(img)\n\u001b[0;32m      4\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrbha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:113\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m pil_image \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not import PIL.Image. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    112\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mThe use of `load_img` requires PIL.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    114\u001b[0m     img \u001b[39m=\u001b[39m pil_image\u001b[39m.\u001b[39mopen(io\u001b[39m.\u001b[39mBytesIO(f\u001b[39m.\u001b[39mread()))\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m color_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    116\u001b[0m         \u001b[39m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[39;00m\n\u001b[0;32m    117\u001b[0m         \u001b[39m# convert it to an 8-bit grayscale image.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ghost.jpg'"
     ]
    }
   ],
   "source": [
    "full_img_path = 'ghost.jpg'\n",
    "encoded_img1 = encode( full_img_path )\n",
    "grad_cam, a_cap1, a_cap2, saliency_img, ace_img, pre_img = explain(full_img_path)\n",
    "encoded_img1 = encoded_img1.reshape(1,1280)\n",
    "\n",
    "x = plt.imread(full_img_path) \n",
    "\n",
    "# plot both saliency and ace images and make it bigger\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 20))\n",
    "\n",
    "# Display each imshow object in a separate subplot\n",
    "axs[0].imshow(saliency_img.get_array(), cmap='jet')\n",
    "axs[1].imshow(ace_img.get_array())\n",
    "axs[2].imshow(x)\n",
    "axs[3].imshow(pre_img.get_array())\n",
    "axs[4].imshow(grad_cam.get_array())\n",
    "axs[0].set_title('Saliency Map')\n",
    "\n",
    "axs[1].set_title('ACE Img')\n",
    "axs[2].set_title('Original Image')\n",
    "axs[3].set_title('Preprocessed Image')\n",
    "axs[4].set_title('Grad-CAM')\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction from model_v1 ::\")\n",
    "print(\"Greedy Search:======>\",greedySearch_v1(encoded_img1,caption_model_v1), beam_search_predictions_v1(encoded_img1,caption_model_v1))\n",
    "print(\"explanation:==>\",a_cap1, a_cap2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(caption_model_v1, to_file=\"my_model.png\", show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4a178f6026637a28f1f634d26a46d599aa51bad03b86b0e8d3f9703468a184b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
